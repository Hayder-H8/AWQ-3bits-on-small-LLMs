{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de43f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Installing packages...')\n",
    "! pip install torch transformers accelerate sentencepiece  datasets tqdm zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d103ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd96741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babcd0ddb6da42c3b2100ff1ea2c616f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7b565fdfe34d5eace41088f03725f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61e70389e134e4b89bcbec478362c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7c11833828474a8d092476d8846ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b001aa85e04313a49be2a918e12fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f7a44d6ce240a780a8ecaeeaf51c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f1b340809a4e8eba57f6d1a94e73c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a1fc5b32a6436aac2ca4ec339b0c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4649b3f08d844f17b1573862a6c515bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d3caa980234f65a7491d2863ac6c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122393be9e924b5eadc7c7c721b7bdbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2249fad33bbc4790b1afcfb7380889bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa48e40f89148209d590da74a7486f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6460ae7dcd7d4d8d846c21e1953168c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e74253e18b147038a59797ed5a58ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"facebook/opt-1.3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cuda\")\n",
    "dataset=load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176672dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=50\n",
    "def evaluate_perplexity(model,tokenizer):\n",
    "    tokenized_data=tokenizer(\"/n/n\".join(dataset['text']),return_tensors='pt')\n",
    "    tokens=tokenized_data.input_ids\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    tokens.to(model.device)\n",
    "    for i in tqdm.tqdm(range(n_samples)):\n",
    "        batch=tokens[:,i*2048:(i+1)*2048].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            logits=model(batch).logits\n",
    "            shift_logits=logits[:,:-1,:].contiguous()\n",
    "            shift_labels=batch[:,1:].contiguous()\n",
    "            loss_fct=nn.CrossEntropyLoss()\n",
    "            loss=loss_fct(shift_logits.view(-1,shift_logits.size(-1)),shift_labels.view(-1))\n",
    "            total_loss+=loss.item()\n",
    "        del batch\n",
    "    return(torch.exp(torch.tensor(total_loss/(n_samples))))\n",
    "\n",
    "def model_size(model,data_width=16,group_size=-1):\n",
    "    if group_size!=-1:\n",
    "        scale_width=16\n",
    "        zero_point_width=4\n",
    "        data_width+= (scale_width + zero_point_width)/group_size\n",
    "    num_params=0\n",
    "    for n,m in model.named_parameters():\n",
    "        num_params+=m.numel()\n",
    "    size_in_bits=num_params*data_width\n",
    "    size_in_megabytes=size_in_bits/(8*1024*1024)\n",
    "    return size_in_megabytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0889e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (in MB): 5019.21875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:15<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model perplexity: 13.209306716918945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Model info \n",
    "print(f'Model size (in MB): {model_size(model,data_width=32,group_size=-1)}')\n",
    "print(f'Model perplexity: {evaluate_perplexity(model,tokenizer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33918aa0",
   "metadata": {},
   "source": [
    "Given a high-precision tensor $W$ (e.g., FP32), uniform quantization aims to represent each weight using a lower bit-width while minimizing the error between $W$ and its quantized version $Q(W)$.\n",
    "\n",
    "Assume that the values of $W$ lie in the range $[\\alpha, \\beta]$ and that we target a bit-width of $b$ bits. The quantized representation is:\n",
    "\n",
    "- Quantized integer: $q$\n",
    "- Scale factor: $S$\n",
    "- Zero Point: $z$\n",
    "\n",
    "The quantized reconstruction is:\n",
    "$$\n",
    "Q(W) = S \\cdot (q - z)\n",
    "$$\n",
    "\n",
    "The scale and zero-point are defined as:\n",
    "$$\n",
    "S = \\frac{\\beta - \\alpha}{2^{b} - 1} \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = -\\text{Round}\\left( \\frac{\\alpha}{S} \\right) \\tag{2}\n",
    "$$\n",
    "\n",
    "Thus, each weight $w \\in W$ is quantized as:\n",
    "$$\n",
    "q(w) = \\text{Clamp}\\left( \\text{Round}\\left(\\frac{w}{S}\\right) + z,\\ 0,\\ 2^{b} - 1 \\right) \\tag{3}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b3d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_quantize_tensor(tensor, num_bits=8, group_size=-1):\n",
    "    original_shape=tensor.shape\n",
    "    if group_size!=-1:\n",
    "        if original_shape[-1]%group_size!=0:\n",
    "            raise ValueError(\"The last dimension of the tensor must be divisible by group_size\")\n",
    "        tensor=tensor.view(-1,group_size)\n",
    "    # alpha and beta calculation\n",
    "    alpha=0\n",
    "    beta=2**num_bits - 1\n",
    "    max_int=beta-alpha\n",
    "    min_val=tensor.min(dim=-1,keepdim=True).values\n",
    "    max_val=tensor.max(dim=-1,keepdim=True).values\n",
    "    scales=(max_val - min_val)/(max_int)\n",
    "    zero_points=torch.round(-min_val/scales).clamp(alpha,beta)\n",
    "    # Quantization\n",
    "    quantized_tensor=torch.round(tensor/scales + zero_points).clamp(alpha,beta)\n",
    "    # Dequantization\n",
    "    dequantized_tensor=(quantized_tensor - zero_points)*scales\n",
    "    return dequantized_tensor.view(original_shape)\n",
    "def pseudo_quantize_model(model, num_bits=4, group_size=-1):\n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data=pseudo_quantize_tensor(m.weight.data,num_bits=num_bits,group_size=group_size)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba7786a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model perplexity: 104.80799865722656\n",
      "Quantized model size (in MB): 495.0596618652344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate quantized model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "pseudo_quantize_model(model, num_bits=3, group_size=128)\n",
    "print(f'Quantized model perplexity: {evaluate_perplexity(model,tokenizer)}')\n",
    "print(f'Quantized model size (in MB): {model_size(model,data_width=3,group_size=128)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ad842",
   "metadata": {},
   "source": [
    "As we can see, after pseudo-quantization to 3 bits with group size 128, the model size is significantly reduced but perplexity rocketed.\n",
    "\n",
    "Let us exmaine , following AWQ logic , the variance of same channels across tokens , and the variance between channels in the same activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c454c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calib_data(tokenizer , n_samples=256 , seq_len=512):\n",
    "    dataset=load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation').shuffle(seed=42)\n",
    "    samples=[]\n",
    "    it = 0\n",
    "    for i in dataset['text']:\n",
    "        i.strip()\n",
    "        it+=1\n",
    "        tokenized=tokenizer.encode(i, return_tensors='pt')\n",
    "        \n",
    "        if tokenized.shape[1]>=seq_len:\n",
    "            continue\n",
    "        sample=torch.tensor(tokenized)\n",
    "                \n",
    "        samples.append(sample)\n",
    "        if it>=n_samples:\n",
    "            print(f'Collected {n_samples} samples for calibration in  {it} iterations')\n",
    "            break\n",
    "    cat_sample=torch.cat(samples,dim=1)\n",
    "    \n",
    "    return  [cat_sample[:,i*seq_len:(i+1)*seq_len] for i in range(cat_sample.shape[1]//seq_len)]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46a0aa06",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 142.12 MiB is free. Process 5898 has 14.60 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 164.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2303229385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5046\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5047\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5048\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5049\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5050\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5468\u001b[0;31m                 \u001b[0m_error_msgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shard_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5469\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error_msgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         disk_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_pointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_param_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_device\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# It is actually not empty!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         to_contiguous, casting_dtype = _infer_parameter_dtype(\n\u001b[1;32m    711\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 142.12 MiB is free. Process 5898 has 14.60 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 164.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\") \n",
    "@torch.no_grad()\n",
    "def plot_activation(model , tokenizer , layer_name , n_samples=512 , seq_len=512):\n",
    "    import matplotlib.pyplot as plt \n",
    "    activations = []\n",
    "    def hook_fn(module, input, output):\n",
    "        print('Hooked layer output shape' , output.shape)\n",
    "        activations.append(output.detach().cpu())\n",
    "    for n, m in model.named_modules():\n",
    "        if n == layer_name:\n",
    "            hook=m.register_forward_hook(hook_fn)\n",
    "            break\n",
    "        raise ValueError(f'Layer {layer_name} not found in the model')\n",
    "    calib_samples = calib_data(tokenizer , n_samples=n_samples , seq_len=seq_len)\n",
    "    print(len(calib_samples))\n",
    "    for sample in calib_samples:\n",
    "        sample = sample.to(model.device)\n",
    "        print(sample.shape)\n",
    "        _=model(sample)\n",
    "    hook.remove()\n",
    "    print('activation of shape' , len(activations))\n",
    "    activations_cat = torch.cat(activations , dim=0)\n",
    "    print('Concatenated activation shape' , activations_cat.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "plot_activation(model , tokenizer , layer_name='model.decoder.layers.0.self_attn.q_proj')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e25a76aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia_smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia_smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2aa6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
